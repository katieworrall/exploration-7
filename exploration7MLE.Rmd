---
title: 'Exploration 7: Maximum Likelihood --- A General Method for Creating Estimators'
author: 'katie worrall'
date: '`r format(Sys.Date(), "%B %d, %Y")`'
header-includes:
  - \usepackage{bm}
output:
  pdf_document:
    number_sections: true
    fig_caption: yes
    fig_height: 8
    fig_width: 8
    latex_engine: xelatex
    citation_package: biblatex
    keep_tex: true
fontsize: 10pt
geometry: margin=1in
graphics: yes
bibliography: classbib.bib
biblio-style: "authoryear-comp,natbib"
---

\input{mytexsymbols}

```{r setup, echo=FALSE, results=FALSE, include=FALSE, cache=FALSE}
library(here)
source(here("rmd_setup.R"))
library(VGAM)
```

Our diplomat friend calls again. This time she feels particularly out of her
depth: "My superiors want a confidence interval for the rate at which Italian
Cabinets fail. They noted that  @cioffirevilla1984pri uses these data for the
1946--1980 period to show that Italian governments end at a constant rate of
.021 per week (i.e the mean length of an Italian cabinet from 1946 to 1980 was
$1/.021 \approx 48$ weeks). But, now we have data until 1987 (there must be
data since then too, but I can't find it when I search online):"


\begin{center}
  \setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}lccccccccccccccccccccc@{}}
\hline
Weeks & 2 & 8 & 12 & 16 & 20 & 24 & 28 & 32 & 40 & 44 & 48 & 52 & 56 & 60 & 64 & 72 &  76 & 88 & 92 & 108 & 180 \\
Number of Cabinets & 4 & 1 & 4 & 1 & 4 & 2 & 6 & 2 & 1 & 1 & 2 & 2 & 2 & 1 & 1 & 2 & 1 & 1 & 1 & 1 & 1 \\
\hline
\end{tabular}
 \end{center}


"Here I convert that table to a data.frame:"

```{r makedata}
Weeks <- c(2, 8, 12, 16, 20, 24, 28, 32, 40, 44, 48, 52, 56, 60, 64, 72, 76, 88, 92, 108, 180)
Num.cabinets <- c(4, 1, 4, 1, 4, 2, 6, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1)
cr.df <- data.frame(duration = rep(Weeks, Num.cabinets))
cr.df
katiesdatframe <- data.frame(Weeks, Num.cabinets)
katiesdatframe
```

And I can even calculate the mean number of weeks between cabinets and convert
it to a failure rate, following Cioffi-Revilla. But, I am flummoxed by the
request for a confidence interval. These are the cabinets in Italy. And this is
a univariate problem. What would statistical inference mean here? What am I
inferring to (in an experiment I'm inferring to an unobserved counterfactual
causal effect, in a sample survey I'm inferring to an unobserved population,
what is unobserved here)? 

>> I think we are inferring to predictions of a rate of failure going forward. The goal here seems to be able to discern a pattern from the past and use it help us better explain and be prepared for what the future may look like. In this case, we are concerned about the failing Italian government cabinets. 

I noticed that in his paper, Cioffi-Revilla reports
confidence intervals and $p$-values but does not re-sample, re-assign, let alone
appeal to a Central Limit Theorem as an approximation to some re-sampling or
re-assigning process to describe either the reference distribution for a test
statistic or the sampling distribution for an estimator. It does appear that  he
does rely on the Central Limit Theorem for statistical inference somehow, but
I'm not sure what we should imagine is being repeated to make a distribution.
What does he do?  What doesn't he observe that he would like to target with
statistical inference? What is he inferring **to**? What is the estimand?"

>>The estimand is the rate at which the cabinets fail. We are trying to infer to a future time. He is trying to observe the rate of failure. However, I don't fully understand the substantive purpose of this rate. What does it actually tell us? How is it more useful to than explaining the causes for the failure of government to be able to assess when failure is more or less likely. How is this rate helpful? 
>>I think the CLT is being used to predict the fitted exponential model for the predicted rates with the observed rates? 

Later she called again, "I have figured out that he says something about a
'data generating process' for a single duration between cabinets that, he
claims, follows the following formula: $pr(Y)=\theta e^{-\theta Y}, \; 0 \le
Y$. What is a data generating process? What does this formula mean in the
context of the durations between cabinets in Italy in this period? Does the
set of durations look at all like they could have arisen from an exponential
distribution with rate $\theta$? Would the `rexp` function in R be
helpful in assessing the general claim about a process producing durations
between cabinets?"

>>We could plot this to figure out if the durations look like exponentail distribution:
>>We are going to generate data that looks like it was generated exponentially to test whether we can actually think of the rate of downfall in terms of an exponential distribution (as opposed to normal, or other...)

```{r}
ggplot(cr.df, aes(duration)) +
  geom_freqpoly()
ggplot(katiesdatframe, aes(Weeks, Num.cabinets)) +
  geom_point()
#this one looks exponential to me
```


"Now, I know that Cioffi-Revilla reports an estimate. So he must have an
estimator. And I assume that he has some idea about a repeated action. But I am
confused. Here are some fragments of notes that I took, can you make sense of
them?"


"The likelihood function is this:

$$\text{Likelihood of }\theta=L(\theta)= p(Y_1,Y_2,\ldots,Y_n|\theta)\text{ and if $Y_i$ iid $L(\theta)$}= p(Y_1|\theta)p(Y_2|\theta)\ldots p(Y_n|\theta)=\prod_{i=1}^n p(Y_i|\theta)$$

But, I don't know what a likelihood function is, or why we would care. It looks
like something to do with a joint probability. And, if we are talking about a
joint probability, I'm confused about what kind of assumptions were required to
go from the big joint probability function to a simple product of probability
density functions or data generating processes. What is going on? What are the
assumptions? Why might we believe them (or not) in this case?"

>> Hmm, I think I am confused about this being joint probability. What two things need to happen at the same time in this case? Isn't this study a test only of when a government falls? Is the other the constant k he mentions as the constant political forces over time? This is a tricky assumption because I think it is quite far lengths to go to say something a varied and volatile as politics is a constant. A major assumption, that I am not quite on board with at this point, is that political government failures can actually be explained through exponential models. A rate of failure is assumed to be a useful predictor for whether a government will fall or not. We assume that the duration of a government is an important signifier for whether a government was successful or a failure (not to mention whether the success or failure was on account of something else). 

"The likelihood that any parameter (or set of parameters) should have any
assigned value (or set of values) is proportional to the probability that if
this were so, the totality of observations should be that observed."
@fisher1922mathematical, page 310 (Fisher first invented likelihood and second
idea of randomization-based inference)

The **maximum likelihood estimate** provides the value of $\theta$ for
which the data provide the most support given the dgp --- that is makes the
observed data 'most probable' or 'most likely'.

>>Theta in this case is the rate of downfall - the maximum likelihood to predict this theta? 

For our data:

$$L(\theta)=\prod \theta e^{-\theta X}$$


```{r deflike}
## The likelihood function
L.exp.fn <- function(theta, y = cr.df$duration) {
  prod(theta * exp(-theta * y))
}
## we could also have used Rs exp pdf fn: prod(dexp(y,rate=theta))
vectorrandom <- c(L.exp.fn(.0001), L.exp.fn(1), L.exp.fn(100), L.exp.fn(.5), L.exp.fn(.000000000001), L.exp.fn(.03), L.exp.fn(.05))
library(ggplot2)
vectordat <- as.data.frame(vectorrandom)
thetas <- c(.0001, 1, 100, .5, .000000000001, .03, .05)
L.exp.fn(.04)
vectordat <- cbind(vectorrandom, thetas)
vectordat <- as.data.frame(vectordat)
vectordat
ggplot(vectordat, aes(thetas, vectorrandom)) +
  geom_point()
prod(dexp(y,rate=.03))
prod(dexp(y,rate=.05))
## it's giving different values for likelihood than the function we created. 
```

I plugged in lots of values for $\theta$ in the R function above and graphed
the result. What should I learn from this graph of the likelihood function?

>> There is a value for theta that produces the highest likelihood, however the likelihood is extremely low. There is also a threshold, with zero on either side of passing the threshold (negative or positive), which means this is not exponential. It is more shaped like a parabola, with the theta being the max or min of the parabola, which you could find with calculus equation I cannot recall. (Wrote this before reading the next part, so I guess I was on to something...)

But, in my reading I see people using the log of the likelihood function, so I
did the following:


\begin{align*}
\logl(\theta)&=\ln \left( \sum \theta e^{-\theta Y_i} \right) \\
&=\sum \ln(\theta e^{-\theta Y_i}) \\
&=\ln(\theta  e^{-\theta Y_1}+\ldots+\theta  e^{-\theta Y_n}) \\
&=\ln(\theta  e^{-\theta Y_1})+\ldots+\ln(\theta  e^{-\theta Y_n})\\
&=\ln(\theta)+\ln(e^{-\theta Y_1})+\ldots+\ln(\theta)+\ln(e^{-\theta Y_n})\\
&=n\cdot \ln(\theta)+(-\theta Y_1 +\ldots+ (-\theta Y_n))\\
&=n \ln(\theta) -\theta \sum Y_i
\end{align*}

If I make another R function for the log of the likelihood function, what
would it look like as I varied $\theta$? Why would I use the log of the
likelihood function versus the likelihood function itself directly? (Although
the math of the logged version certainly looks simpler). It almost looks as if
it would be \emph{sufficient} to have $n$ and the total durations to calculate
values of the log-likelihood function and that we wouldn't need the raw data
at all. Is this right? Is this what people mean by "sufficient statistic"? How
is this useful?

>> The log likelihood function has much simpler? numbers. That is, when we used just the likelihood function, I would get extremely close to zero numbers that made it difficult to discern a change in likelihood. With log, the numbers are within 0-1000, so they are easier to read and understand. However, with this being a likelihood function, I still have trouble interpreting a number like 304 in terms of likelihood? 
>> Based on a source I found online (referenced below), the sufficient statistic is when the random sample is a close match to when the random sample is unknown, but with a specific parameter. In this case, we do have the parameter, and from what I understand, a parameter is some distribution form. We can just randomly predict data around this distribution (in our case, exponential), and thus, yes, it seems the need for raw data becomes obsolete. It seems useful when you don't know much about a random sample, or when you have missing data in between two time periods and need to fill it in, or when you want to be able to predict a pattern. 

Now, most people don't seem to graph curves or surfaces of their likelihood
functions (logged or otherwise). But I know that there is something special
about the maximum of the log likelihood function (or perhaps the minimum of the
negative log likelihood function). What is it? How would I find the maximum of
the log likelihood function (i.e. is there a general formula for $\theta$ where
the log likelihood function here takes on its maximum value?)?^[She confesses
that she doesn't remember too much calculus so she used Wolfram Alpha online to
do: `Solve[D[ n * Log[t] - t * y, t]==0,t]`. Or use Sage
<http://www.sagemath.org/> or <https://cloud.sagemath.com>  to do something similar. ]

So, we can find the maximum by just graphing the curve, or by using calculus.
Another way is to find the maximum numerically doing something like:

```{r, eval=FALSE}

## The log likelihood function. Something is causing an error here. How to fix it?
Log.L.exp.fn <- function(theta, y = cr.df$duration) { ## Theta can be a vector
  n <- length(y)
  sumy <- sum(y)
  if (any(theta <= 0)) {
    ## This makes values for theta less than or equal to zero really not near the maximum
    return(-99999)
  } else {
    return(n*log(theta)-theta*sumy)
    return(sum(dexp(cr.df$duration,rate=theta,log=TRUE)))
    return(sum(log(dexp(y, rate = theta, log = FALSE))))
  }
}
lograndom <- Log.L.exp.fn(c(.000001, .00001, .001, .01 ,.021, .03, .05, .06, .07))
logthetas <- c(.000001, .00001, .001, .01,.021, .03, .05, .06, .07)
logs <- cbind(lograndom, logthetas)
logs <- as.data.frame(logs)
ggplot(logs, aes(logthetas, lograndom)) +
  geom_point()
## What is this doing?
themle.max <- optim(par = c(theta = 0), Log.L.exp.fn, control = list(fnscale = -1, trace = 2), method = "BFGS", hessian = TRUE)
#You are finding the maximum on the ln curve as shown in the graph below. However, we are not given theta, but the MLE. Could we create function that gives us theta when we have MLE? 
#MLE is y, and theta is x, so I could plug it into a log graph right? 
#y = log(x)
exp(192.443577)
#nope
```

```{r}
log.L.exp.fn <- function(theta, y = cr.df$duration) {
  log(prod(theta * exp(-theta * y)))
}
lograndom1 <- c(log.L.exp.fn(.000001), log.L.exp.fn(.001), log.L.exp.fn(.01), log.L.exp.fn(.03), log.L.exp.fn(.05), log.L.exp.fn(.06), log.L.exp.fn(.07))
logthetas1 <- c(.000001, .00001, .001, .01, .03, .05, .06, .07)
logs1 <- cbind(lograndom1, logthetas1)
logs1 <- as.data.frame(logs1)
ggplot(logs1, aes(logthetas1, lograndom1)) +
  geom_point()
```

Do all the methods agree?

>> They all have the same theta for MLE to be around .03, which makes me think all these methods agree. 

It is cool, of course, that we have generated an estimator (i.e. we came up
with a formula to make a good guess (?how good? ?how would we know?) about
something unobserved that we care about) using only statements about the kind
of probability machine that we think could produce the outcome. But, what about
statistical inference? We need a sampling distribution. Where would this come
from? Someone mentioned "use the CLT"? What would that mean? It seems like they
just want me to claim that this maximum likelihood estimator produces estimates
that are like means or sums of independent observations and that thus we know
that the sampling distribution will be Normal. Is this right?  If so, then how
would I know something about the spread or center of this Normal distribution?
Is this estimator unbiased? If not, what are its properties such that I can
know which Normal distribution to use for statistical inference."

>> I am not sure we would be using a normal distibution here since he is trying to figure out a rate. Also, the equation he is using is an exponential equation R = e^-kt. How would the CLT be helpful here? We would know the spread of observations based on the exponential curved line- this is how the durations are predicted to come up with the rate of failure. The mean is used here to calculate the rate, or R in the equation. Based on the graphs, this estimator doesn't look biased because all the plots are pretty similar. 

Even if I thought that the center of the distribution of $\hat{\theta}$ is
somehow centered on $\theta$, I am wondering about how to represent the idea
that a larger sample is more information (and that thus a larger sample should
have a narrower Normal distribution and shorter confidence intervals).

One thing that I played around with is the following:

  Look at the following four log-likelihood functions with the same
  maximum of $\hat{\theta}=.025$. The vertical black line is at the
  mle estimate. The gray horizontal lines are at the maximum of each
  likelihood function.

```{r morellfns}
Log.L.exp.n10.fn <- function(theta = x) {
  10 * log(theta) - theta * 400
}
Log.L.exp.n20.fn <- function(theta = x) {
  20 * log(theta) - theta * 800
}
Log.L.exp.n41.fn <- function(theta = x) {
  41 * log(theta) - theta * 1640
}
Log.L.exp.n100.fn <- function(theta = x) {
  100 * log(theta) - theta * 4000
}

proposed.thetas <- seq(.01, .06)

## Setup the y-axis range so all three likelihood functions can go on the same plot.
yrange <- range(
  Log.L.exp.n10.fn(theta = proposed.thetas),
  Log.L.exp.n20.fn(theta = proposed.thetas),
  Log.L.exp.n41.fn(theta = proposed.thetas),
  Log.L.exp.n100.fn(theta = proposed.thetas)
)
```


```{r moreloglikplots,fig.width=4,fig.height=4,out.width='.6\\textwidth'}
par(mfrow = c(1, 1))
curve(Log.L.exp.n100.fn(theta = x),
  from = .01, to = .06, ylim = yrange, ylab = "Log-Likelihood",
  xlab = expression(theta)
)
curve(Log.L.exp.n41.fn(theta = x), from = .01, to = .06, ylim = yrange, add = TRUE, col = "green")
curve(Log.L.exp.n20.fn(theta = x), from = .01, to = .06, ylim = yrange, add = TRUE, col = "blue")
curve(Log.L.exp.n10.fn(theta = x), from = .01, to = .06, ylim = yrange, add = TRUE, col = "red")
abline(v = c(.025, .02), col = c("black", "gray"))
abline(
  h = c(
    Log.L.exp.n100.fn(theta = .025),
    Log.L.exp.n41.fn(theta = .025),
    Log.L.exp.n20.fn(theta = .025),
    Log.L.exp.n10.fn(theta = .025)
  ),
  col = "gray"
)

text(
  rep(.05, 4),
  c(
    Log.L.exp.n100.fn(theta = .05),
    Log.L.exp.n41.fn(theta = .05),
    Log.L.exp.n20.fn(theta = .05),
    Log.L.exp.n10.fn(theta = .05)
  ),
  c("n=100", "n=41", "n=20", "n=10")
)
```

If you took a proposed $\theta=.025$ and another value close by, like
.02 (shown by the vertical gray line), and you subtracted the value of
the likelihood function at .025 from the value at .02 or .01, which
likelihood function would produce the largest difference? What aspects of the
curves seem to capture the idea of "information"?

>> Is theta the rate of failure? What is theta?

```{r }

## The log likelihood functions evaluated at the maximum
themle <- c(
  n100 = Log.L.exp.n100.fn(theta = .025),
  n41 = Log.L.exp.n41.fn(theta = .025),
  n20 = Log.L.exp.n20.fn(theta = .025),
  n10 = Log.L.exp.n10.fn(theta = .025)
)

## The log likelihood functions evaluated close to the maximum
notmlebutclose <- c(
  Log.L.exp.n100.fn(theta = .02),
  Log.L.exp.n41.fn(theta = .02),
  Log.L.exp.n20.fn(theta = .02),
  Log.L.exp.n10.fn(theta = .02)
)

themle - notmlebutclose ## The difference in log likelihood.
```

I want some measure of the curve in the likelihood around the maximum. What is
this "hessian" thing that I've seen people talk about online? How can it help
us here?

>>I found  that a hessian matrix helps us figure out "saddle points and local extremum" (Machine Learning Mastery) like the maximum of the curve that we are searching for. This will help us determine theta, right now we only have the MLE, right?

So, what have we done? We have an estimate and a confidence interval. But what
do they mean? It seems as if the answer has to do not with repeated sampling
from some population or repeated assigning of some treatment, but with the
"population" referring to all the ways that the
outcome-value-generating-machine could produce the values (here the machine is
an exponential machine --- we plug in some values and it produces other
values, but the machine is stochastic, it is a probability density function).
How can I understand what is going on here? It is clearly very cool that if I
can articulate a dgp and also say how the individual dgp's go together (here
they are all independent and the same and so I just multiple them)
 >>What does i mean for the dgp's to be independent? Independent from what? 
 
, then I can
write a likelihood function, and have a useful estimate of a parameter be the
value of the parameter at the maximum of the function AND the standard error
arising from the curvature of the function at the maximum. I'm just wondering
how to communicate about this approach with my bosses. Thanks much for your
help!"

>> We have all these historical observations of the Iatlian government staying in power for a certain amount of weeks, and then failing. We have assumed that political forces act as constant, k. We want to understand this pattern of failure so we could have a better sense of what failure will look for Italian government cabinets going forward. However, we do not want just any rate that we could calculate from the mean of weeks and the number of cabinets. This rate must follow a pattern, and interestingly, the rate at which the Italian government failed looked like an exponential graph - as in many governments were only in power for short amounts of time, and not many governments actually stayed in power a long time. Whether this is just coincidence of the data, or just inuitively makes sense, is another question, but we need to calculate a "political reliability rate" because that is what we have been asked to do. We use maximum likelihood in terms of a reference distribution. You can look at a curve and assess the maximum likelihood of a certain observation under that curve. This is a bit easier for me to understand with a Normal distribution, and understanding it in terms of this exponential distribution is trickier. I believe obtaining the MLE helps us get the rate of failure, which is the point at which the exponential graph turns from vertical to horizonatl line, or I suppose curves and changes direction. Because our observed values seemed to match our predicted values with the exponential distribution, we can count on this as a reliable model. We can also calculate standard errors and mean differences from our observed and predicted model. 

## Covariates

"Someone then says, 'Explanatory variables like interventions as well as
covariates can be used in MLE by parameterizing your likelihood function.'
Again, I found this puzzling. So, I asked for code. But the code they sent was
difficult for me to understand. Can you explain how what is going on is
'parameterizing' a likelihood function and how it works to enable us to 'use'
covariates and explanatory variables to learn about relationships? What are we
learning about when we do this anyway? Can you interpret the code and the results below?"

```{r parameterizedlikelihood}
load(url("http://www.jakebowers.org/PS531Data/cabinet-data.rda"))
## x-1 means maj=0 and else=1, 1-(x-1) means maj=1 else=0
cabdata$numstatus <- 1 - (cabdata$NUMST2 - 1)
## No fractional durations. Count number of weeks.
cabdata$durnew <- ifelse(cabdata$DURAT < 1, 0, cabdata$DURAT)
themf <- model.frame(durnew ~ CRISIS + numstatus + ITALY, data = cabdata)
X <- model.matrix(durnew ~ CRISIS + numstatus + ITALY, data = themf)
y <- model.response(themf)


## Modeling *number of days* or *count* of days.
pois_log_lik <- function(theta, y, X) {
  mu <- X %*% theta
  if (any(mu <= 0)) {
    ## mu cannot be less than 0
    return(-99999)
  }
  ll <- sum(dpois(t(y), lambda = exp(mu), log = TRUE))
  # ll <- sum(log(dpois(t(y), lambda =  exp(mu), log = FALSE)))
  ll
}

starting_values1 <- c(1 / mean(cabdata$durnew), .01, .01, .01)
names(starting_values1) <- colnames(X)
starting_values2 <- c(0, 0, 0, 0)
names(starting_values2) <- colnames(X)
## Testing the functions
pois_log_lik(theta = starting_values1, y = y, X = X)
pois_log_lik(theta = starting_values2, y = y, X = X)
pois_log_lik(theta = starting_values2 + .01, y = y, X = X)

themle_pois <- optim(
  par = starting_values1, y = y, X = X, fn = pois_log_lik, hessian = TRUE, method = "BFGS",
  control = list(fnscale = -1, trace = TRUE)
)
themleses_pois <- sqrt(diag(solve(-1 * themle_pois$hessian[1:4, 1:4])))
themlecoefs_pois <- cbind(mlebhat = themle_pois$par[1:4], mlesehat = themleses_pois[1:4])

## How sensitive are these results to starting values?
themle_pois2 <- optim(
  par = c(1, 0, 0, 0), y = y, X = X, fn = pois_log_lik, hessian = TRUE, method = "BFGS",
  control = list(fnscale = -1, trace = TRUE)
)
themle_pois3 <- optim(
  par = c(.1, 1, 1, 1), y = y, X = X, fn = pois_log_lik, hessian = TRUE, method = "BFGS",
  control = list(fnscale = -1, trace = TRUE)
)
themle_pois$par
themle_pois2$par
themle_pois3$par

## hmmm.... fitdistrplus ?? https://cran.r-project.org/web/packages/fitdistrplus/vignettes/paper2JSS.pdf
## https://github.com/petrkeil/Statistics/blob/master/Lecture%203%20-%20poisson_regression/poisson_regression.Rmd
glm1 <- glm(durnew ~ CRISIS + numstatus + ITALY, family = poisson(link = "log"), data = cabdata)
vglm1 <- vglm(durnew ~ CRISIS + numstatus + ITALY, poissonff(link = "loglink"), data = cabdata)
```

Useful reading:

 - \citealp{ward2018maximum} (probably the best one)

 - \citealp[Chap 9.3.3]{fox2008applied}

 - \citealp[Use the 2009 Version of]{green1991mle}  from \url{https://sites.google.com/site/donaldpgreen/plsc504}

 - \citealp[Chap 5]{fox2011r} and see also \url{http://socserv.socsci.mcmaster.ca/jfox/Courses/SPIDA/index.html}

 - \citealp[Chap 4]{king89}

 - \citealp[Chap 1,2]{cox:2006}


# References
https://www.math.arizona.edu/~tgk/466/sufficient.pdf
https://machinelearningmastery.com/a-gentle-introduction-to-hessian-matrices/
